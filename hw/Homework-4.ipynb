{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 1:__ Linear SVM\n",
    "\n",
    "*Data Processing (see code in homework4.py)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.metrics \n",
    "\n",
    "\"\"\" DATA PROCESSING \"\"\"\n",
    "\n",
    "# access data, concatenate to split later \n",
    "df_train = pd.read_csv('https://web.stanford.edu/~hastie/ElemStatLearn/datasets/vowel.train')\n",
    "df_test = pd.read_csv('https://web.stanford.edu/~hastie/ElemStatLearn/datasets/vowel.test')\n",
    "df = pd.concat([df_train, df_test])\n",
    "\n",
    "# drop extra index \n",
    "df = df.drop('row.names', axis=1)\n",
    "\n",
    "# select '1' as positives and '2', '3' as negatives, drop rest, apply +/- 1 labels\n",
    "df = df[df.y.between(1, 3, inclusive=True)] \n",
    "df.y = df.y.apply(lambda x: 1 if x==1 else -1)\n",
    "\n",
    "# splitting data (70/15/15 ratio)\n",
    "n = len(df)\n",
    "\n",
    "# note: data is ordered in increasing pattern by response\n",
    "# (y=1, 2, ... 11, 1, 2, ... 11), hence split automatically provides \n",
    "# desired similarity in positive/negative-proportions for each set \n",
    "train, validate, test = np.split(df, [int(0.7*n), int(0.85*n)])\n",
    "\n",
    "y_train = train.y.to_numpy()\n",
    "y_val = validate.y.to_numpy()\n",
    "y_test = test.y.to_numpy()\n",
    "\n",
    "X_train = train.drop('y', axis=1).to_numpy()\n",
    "X_val = validate.drop('y', axis=1).to_numpy()\n",
    "X_test = test.drop('y', axis=1).to_numpy()\n",
    "\n",
    "# standardize based off of training set \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Gradient Derivation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our objective function, our gradient function is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla F(\\beta)= 2\\lambda\\beta\\:+\\frac{\\rho}{n}\\sum_{i=1;\\:y=+1}^{n}{\\nabla \\ell_{hh}(y_i,x_i^T\\beta)}\\:+\\:\\frac{1-\\rho}{n}\\sum_{i=1;\\:y=-1}^{n}{\\nabla \\ell_{hh}(y_i,x_i^T\\beta)}\n",
    "\\end{equation}\n",
    "\n",
    "Where $\\nabla \\ell_{hh}(y_i,x_i^T\\beta)$ is as follows. __*Note:*__ to differentiate with respect to $\\beta$, it is necessary to represent the $t$ term in $\\nabla \\ell_{hh}(y, t)$ as $x_i^T\\beta$.\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla \\ell_{hh}(y_i, x_i^T\\beta) := \\left\\{\n",
    "                                        \\begin{array}{ll}\n",
    "                                            0 & \\quad \\text{if}\\:\\:y_i x_i^T\\beta > 1+h \\\\\n",
    "                                            \\frac{1}{2h}(1+h-y_i x_i^T\\beta)(-y_i x_i) & \\quad \\text{if}\\:\\:|1-y_i x_i^T\\beta| \\leq h \\\\\n",
    "                                            -y_i x_i & \\quad \\text{if}\\:\\:y_i x_i^T\\beta < 1-h\n",
    "                                        \\end{array}\n",
    "                                     \\right.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Function Definitions (see code in homework4.py)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" FUNCTIONS \"\"\"\n",
    "\n",
    "def huber_loss(y, t, h=0.5):\n",
    "    lhh = None\n",
    "    if y*t > 1+h:\n",
    "        lhh = 0\n",
    "    elif np.abs(1-(y*t)) <= h:\n",
    "        lhh = ((1+h-(y*t))**2) / (4*h)\n",
    "    elif y*t < 1-h:\n",
    "        lhh = 1-(y*t)\n",
    "    else: \n",
    "        print('Parameter failure in Huber loss function.')\n",
    "    return lhh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_huber_loss(y, X, B, h=0.5):\n",
    "    # expect col. vector X\n",
    "    t = np.dot(X,B)\n",
    "    grad_lhh = None\n",
    "    if y*t > 1+h:\n",
    "        grad_lhh = np.zeros(X.shape)\n",
    "    elif np.abs(1-(y*t)) <= h:\n",
    "        grad_lhh = (1/(2*h)) * (1+h-(y*t)) * (-y*X) \n",
    "    elif y*t < 1-h:\n",
    "        grad_lhh = -y*X\n",
    "    else: \n",
    "        print('Parameter failure in Huber loss function.')\n",
    "    return grad_lhh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj(B, rho, lmbda, X, y):\n",
    "    penalty = lmbda * np.linalg.norm(B)**2\n",
    "    pos_term = 0\n",
    "    neg_term = 0 \n",
    "    \n",
    "    for i in np.where(y == 1)[0]:\n",
    "        pos_term += huber_loss(y[i], np.dot(X[i], B))\n",
    "    \n",
    "    for i in np.where(y == -1)[0]:\n",
    "        neg_term += huber_loss(y[i], np.dot(X[i], B)) \n",
    "    \n",
    "    pos_term *= rho/len(y)\n",
    "    neg_term *= (1-rho)/len(y)\n",
    "    return penalty + pos_term + neg_term "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(B, rho, lmbda, X, y):\n",
    "    penalty = 2*lmbda*B\n",
    "    pos_term = np.zeros(X.shape[1])\n",
    "    neg_term = np.zeros(X.shape[1])\n",
    "    \n",
    "    for i in np.where(y == 1)[0]: \n",
    "        pos_term += grad_huber_loss(y[i], X[i], B)\n",
    "    \n",
    "    for i in np.where(y == -1)[0]:\n",
    "        neg_term += grad_huber_loss(y[i], X[i], B)\n",
    "    \n",
    "    pos_term *= rho/len(y)\n",
    "    neg_term *= (1-rho)/len(y)\n",
    "    return penalty + pos_term + neg_term "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking(B, rho, lmbda, X, y, eta=1, alpha=0.5, gamma=0.8, max_iter=100):\n",
    "    grad_B = grad(B, rho, lmbda, X, y)\n",
    "    norm_grad_B = np.linalg.norm(grad_B)\n",
    "    found_eta = 0\n",
    "    num_iters = 0\n",
    "    \n",
    "    while found_eta == 0 and num_iters < max_iter:\n",
    "        if obj(B-eta*grad_B, rho, lmbda, X, y) < obj(B, rho, lmbda, X, y)-alpha*eta*norm_grad_B**2:\n",
    "            found_eta = 1\n",
    "        elif num_iters == max_iter:\n",
    "            raise('Max. iteration of BLS reached.')\n",
    "        else:\n",
    "            eta *= gamma\n",
    "            num_iters += 1 \n",
    "            \n",
    "    return eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mylinearsvm(rho, lmbda, X, y, eps=5e-3):\n",
    "    # init eta using L constant as described in class \n",
    "    w, v = np.linalg.eigh((1/len(y)) * np.dot(X.T, X))\n",
    "    \n",
    "    beta = np.zeros(X.shape[1])\n",
    "    theta = np.zeros(X.shape[1])    \n",
    "    eta = 1 / (max(w) + lmbda) \n",
    "    \n",
    "    grad_theta = grad(theta, rho, lmbda, X, y)\n",
    "    \n",
    "    grad_beta = grad(beta, rho, lmbda, X, y)\n",
    "    \n",
    "    beta_vals = beta \n",
    "    t = 0 \n",
    "    \n",
    "    while np.linalg.norm(grad_beta) > eps:\n",
    "        # hw3 soln used theta instead of beta as arg here -- unsure if error\n",
    "        eta = backtracking(beta, rho, lmbda, X, y, eta=eta) \n",
    "        beta_new = theta - eta*grad_theta \n",
    "        theta = beta_new + (t/(t+3)) * (beta_new-beta)\n",
    "        \n",
    "        beta_vals = np.vstack((beta_vals, beta))\n",
    "        grad_theta = grad(theta, rho, lmbda, X, y)\n",
    "        grad_beta = grad(beta, rho, lmbda, X, y)\n",
    "        beta = beta_new\n",
    "        t += 1 \n",
    "    \n",
    "    return beta_vals "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Training Performance ($\\lambda=1, \\rho=1$)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General misclassification error is: 21.16%.\n",
      "Achieved sensitivity is: 87.30%.\n",
      "Achieved specificity is: 74.60%.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" TRAINING & TESTING \"\"\"\n",
    "\n",
    "# rho=1, lambda=1\n",
    "final_beta = mylinearsvm(1, 1, X_train, y_train)[-1]\n",
    "\n",
    "def performance_metrics(B, X, y, output=True):\n",
    "    P = len(np.flatnonzero(y==1))\n",
    "    N = len(np.flatnonzero(y==-1))\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "        \n",
    "    for i in range(len(y)):\n",
    "        if int(np.sign(np.dot(X[i], final_beta))) == 1:\n",
    "            TP = TP+1 if y[i]==1 else TP\n",
    "            FP = FP+1 if y[i]==-1 else FP\n",
    "        else:\n",
    "            TN = TN+1 if y[i]==-1 else TN\n",
    "            FN = FN+1 if y[i]==1 else FN \n",
    "    \n",
    "    misclassed = 100*(FP+FN)/len(y)\n",
    "    sens = 100*TP/P\n",
    "    spec = 100*TN/N\n",
    "    \n",
    "    if output:\n",
    "        print('General misclassification error is: %.2f%%.' % misclassed)\n",
    "        print('Achieved sensitivity is: %.2f%%.' % sens)\n",
    "        print('Achieved specificity is: %.2f%%.' % spec)\n",
    "            \n",
    "    return (misclassed, sens, spec)\n",
    "\n",
    "\n",
    "# feedback on rho=1, lambda=1\n",
    "metrics = performance_metrics(final_beta, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Training Performance ($\\lambda=1, \\rho=0.1, 0.2, ..., 0.9, 1.0$)*\n",
    "\n",
    "A recurring problem emerges (and is found in others' work, as well). The variation in $\\rho$ seems to have a negligible effect on $\\beta$, and hence a negligible effect on all performance metrics. Outside of derivation and coding issues, it is believed that qualities inherent to the data may be causing this behavior. Hence, the plotted lines are nearly flat (they increase slightly for the much larger values of $\\rho$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lambda=1, rho=0.1 to 1.0\n",
    "rhos = [x/10 for x in list(range(1,11))]\n",
    "lmbda = 1 \n",
    "\n",
    "misclass_errs = []\n",
    "sensitivities = []\n",
    "specificities = [] \n",
    "\n",
    "for rho in rhos:\n",
    "    final_beta = mylinearsvm(rho, lmbda, X_train, y_train)[-1]\n",
    "    metrics = performance_metrics(final_beta, X_train, y_train, output=False)\n",
    "    misclass_errs.append(metrics[0]/100)\n",
    "    sensitivities.append(metrics[1]/100)\n",
    "    specificities.append(metrics[2]/100)\n",
    "\n",
    "plt.plot(rhos, misclass_errs, label=\"Misclassification Error\")\n",
    "plt.plot(rhos, sensitivities, label=\"Sensitivity\")\n",
    "plt.plot(rhos, specificities, label=\"Specificity\")\n",
    "plt.title('Performance Metrics vs. Imbalance Parameter')\n",
    "plt.xlabel('Imbalance Parameter')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Varying $\\lambda$ & $\\rho$*\n",
    "\n",
    "We tune the algorithm along two dimensions: $\\lambda=0.1, 0.2, ..., 0.9, 1.0$ and $\\rho=0.1, 0.2, ..., 0.9, 1.0$. Our grid of values for $\\lambda$ and $\\rho$ is hence of size $10\\:x\\:10$. To calculate area-under-the-curve, where the ROC curve is defined as $\\frac{sensitivity}{1-specificity}$, we use *__sklearn.metrics.roc_auc_score__*. The pair of $\\lambda$ and $\\rho$ values that optimize area-under-the-curve for the *__validation data__*, and the optimal AUC value itself, are as follow. Lastly computed is the sensitivity and specificity for this pair of values, on the *__testing set__*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the validation set:\n",
      "\n",
      "The optimal value of lambda is: 0.1.\n",
      "The optimal value of rho is: 0.1.\n",
      "The corresponding, optimized area-under-the-ROC-curve is: 0.98.\n",
      "\n",
      "For the testing set (using above optimal parameters):\n",
      "\n",
      "The achieved sensitivity is: 38.46%\n",
      "The achieved specificity is: 85.71%\n"
     ]
    }
   ],
   "source": [
    "# varying both lambda and rho\n",
    "lambdas = rhos \n",
    "\n",
    "def predict(B, X=X_val, y=y_val):\n",
    "    predictions=np.array([])\n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        predictions = np.append(predictions, int(np.sign(np.dot(X[i], B))))\n",
    "        \n",
    "    return predictions \n",
    "\n",
    "\n",
    "# makeshift grid search \n",
    "optimal_roc_auc = 0\n",
    "optimal_rho = -1\n",
    "optimal_lambda = -1 \n",
    "optimal_B = -1\n",
    "\n",
    "for l in lambdas:\n",
    "    for r in rhos: \n",
    "        Bf = mylinearsvm(r, l, X_val, y_val)[-1]\n",
    "        y_preds = predict(Bf)\n",
    "        score = sklearn.metrics.roc_auc_score(y_val, y_preds)\n",
    "        \n",
    "        if score > optimal_roc_auc:\n",
    "            optimal_roc_auc = score\n",
    "            optimal_rho = r\n",
    "            optimal_lambda = l\n",
    "            optimal_B = Bf\n",
    "            \n",
    "\n",
    "print('For the validation set:\\n\\nThe optimal value of lambda is: %.1f.' % optimal_lambda)\n",
    "print('The optimal value of rho is: %.1f.' % optimal_rho)\n",
    "print('The corresponding, optimized area-under-the-ROC-curve is: %.2f.' % optimal_roc_auc)\n",
    "\n",
    "metrics = performance_metrics(optimal_B, X_test, y_test, output=False)\n",
    "print('\\nFor the testing set (using above optimal parameters):\\n')\n",
    "print('The achieved sensitivity is: %.2f%%' % metrics[1])\n",
    "print('The achieved specificity is: %.2f%%' % metrics[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3__\n",
    "\n",
    "*Linear Regression Insomnia*\n",
    "\n",
    "For Bobbie, the time taken to train the model will play a significant part in the decision regarding which optimization algorithm he should use. That means if he is training a linear regression model (say, using gradient descent), I would recommend he use the *__Fast Gradient__* algorithm as discussed in class. This includes the use of *backtracking line search* for an adaptive step-size, and has the general advantage of converging to desirable solution faster than standard gradient descent algorithm. Note that if possible, I also recommend that Bobbie incorporate some form of *penalized/regularized* linear regression, since he is also tasked with validating the model on a dataset. This way, his model is discouraged from overfitting, without adding much time to the process. \n",
    "\n",
    "*The Next Big Thing*\n",
    "\n",
    "I would not invest anything into Joey's company. The inclusion of the test set during training will naturally lead to a poorly-generalizable model. The model (never tested) will follow the noise of availabe samples too closely, or overfit. The combining of the data means that while Joey may predict his testing samples perfectly, the variance of his model will be high. That is, performance on other datasets will be undoubtedly inconsistent. I will urgently relay to Joey that, clearly, this means his model is not ready for real-world use. I will then re-evaluate why Joey has stayed my colleague for so long. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
