{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__*Data Competition-Related Exercise*__\n",
    "\n",
    "a) Downloading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "X_train = np.load('train_features.npy')\n",
    "X_val = np.load('test_features.npy')\n",
    "X_test = np.load('val_features.npy')\n",
    "\n",
    "y_train = np.load('train_labels.npy')\n",
    "y_val = np.load('val_labels.npy')\n",
    "\n",
    "# standardize data \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Selecting two classes (those with indexes 0,1), whose respective labels are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick classes \n",
    "FILTERED_CLASSES = {0: 'n01820546', 1: 'n01824575'}\n",
    "\n",
    "# filter data\n",
    "train_idxs = np.where(np.in1d(y_train, [0,1]))\n",
    "y_train = y_train[train_idxs]\n",
    "X_train = X_train[train_idxs]\n",
    "\n",
    "val_idxs = np.where(np.in1d(y_val, [0,1]))\n",
    "y_val = y_val[val_idxs]\n",
    "X_val = X_val[val_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next find the optimal $\\lambda$ using `LogisticRegressionCV` from `sklearn`, with a 5-fold setting. Note that we are using the objective function defined in class, which is not identical to the objective function in `sklearn`. \n",
    "\n",
    "We find that the value $C$ returned by `sklearn` is equivalent to $\\frac{1}{2n\\lambda}$ in our derivation, so the necessary transformation is applied, and the $\\lambda$ value is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using sklearn CV \n",
    "clf = LogisticRegressionCV(cv=5, fit_intercept=False).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal lambda, after 5-fold CV via skearn: 0.0108\n"
     ]
    }
   ],
   "source": [
    "# finding lambda\n",
    "optimal_lmbda = 1/(2*clf.C_[0]*len(y_train)) \n",
    "print('Optimal lambda, after 5-fold CV via skearn: %.4f' % optimal_lmbda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) We define the necessary functions (pulled from previous homework submissions) and perform training with our fast gradient algorithm. Misclassification error is provided (reasonably low for the training set, high for the validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj(B, lmbda, X=X_train, y=y_train):\n",
    "    risk = (1/len(y)) * np.sum(np.log(1 + np.exp(-y*np.dot(X,B))))\n",
    "    penalty = lmbda * np.linalg.norm(B)**2\n",
    "    return risk + penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(B, lmbda, X=X_train, y=y_train):\n",
    "    yx = y[:, np.newaxis]*X\n",
    "    B_shaped = B[:, np.newaxis]\n",
    "    denom = (1+np.exp(np.dot(-yx,B)))[:, np.newaxis]\n",
    "    \n",
    "    # combined formula \n",
    "    risk = (1/len(y)) * np.sum(-yx*np.exp(np.dot(-yx, B_shaped)) / denom, axis=0)\n",
    "    penalty = 2*lmbda*B\n",
    "    return risk + penalty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking(B, lmbda, eta=1, alpha=0.5, gamma=0.8, max_iter=100, X=X_train, y=y_train):\n",
    "    grad_B = grad(B, lmbda, X=X, y=y)\n",
    "    norm_grad_B = np.linalg.norm(grad_B)\n",
    "    finished_bt = False\n",
    "    t = 0\n",
    "    \n",
    "    while (not finished_bt) and t < max_iter:\n",
    "        if obj(B-eta*grad_B, lmbda, X=X, y=y) < obj(B, lmbda, X=X, y=y)-alpha*eta*norm_grad_B**2:\n",
    "            finished_bt = True\n",
    "        elif t == max_iter:\n",
    "            raise \"Exceeding 100 iterations in backtracking line search.\"\n",
    "        else:\n",
    "            eta *= gamma\n",
    "            t += 1 \n",
    "    \n",
    "    return eta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastgradient(B_init, Th_init, lmbda, eta, X=X_train, y=y_train, epsilon=5e-3):\n",
    "    B = B_init\n",
    "    Th = Th_init\n",
    "    grad_B = grad(B, lmbda, X=X, y=y)\n",
    "    grad_Th = grad(Th, lmbda, X=X, y=y)\n",
    "    \n",
    "    Bs = B\n",
    "    t = 0\n",
    "    \n",
    "    while np.linalg.norm(grad_B) > epsilon:\n",
    "        eta = backtracking(Th, lmbda, eta=eta, X=X, y=y)\n",
    "        B_next = Th - eta*grad_Th\n",
    "        Th = B_next + (t/(t+3))*(B_next-B)\n",
    "        \n",
    "        Bs = np.vstack((Bs, B))\n",
    "        grad_Th = grad(Th, lmbda, X=X, y=y)\n",
    "        grad_B = grad(B, lmbda, X=X, y=y)\n",
    "        B = B_next\n",
    "        t += 1\n",
    "            \n",
    "    return Bs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training \n",
    "d = X_train.shape[1]\n",
    "B_init = np.zeros(d)\n",
    "Th_init = np.zeros(d)\n",
    "\n",
    "e = scipy.linalg.eigh(1/len(y_train)*np.dot(X_train.T, X_train), eigvals=(d-1,d-1), eigvals_only=True)\n",
    "e = e[0] + optimal_lmbda \n",
    "eta_init = 1/e\n",
    "\n",
    "B = fastgradient(B_init, Th_init, optimal_lmbda, eta_init)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-training misclassification error, training set: 21.80%.\n",
      "Post-training misclassification error, validation set: 50.50%.\n"
     ]
    }
   ],
   "source": [
    "def misclassification(B, X, y):\n",
    "    y_pred = 1/(1+np.exp(-X.dot(B))) > 0.5\n",
    "    y_pred = 1*y_pred\n",
    "    return np.mean(y_pred != y)\n",
    "\n",
    "trn_err = misclassification(B, X_train, y_train)*100\n",
    "val_err = misclassification(B, X_val, y_val)*100 \n",
    "\n",
    "print('Post-training misclassification error, training set: %.2f%%.' % trn_err)\n",
    "print('Post-training misclassification error, validation set: %.2f%%.' % val_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) All code was run on AWS. Please see provided screenshots. Since a notebook (and not a script) was used, its results are used in place of output screenshots.\n",
    "\n",
    "e) We plot misclassification error vs. $\\lambda$ for the testing and training sets. Some notes:\n",
    "    - Only 3 submissions are possible each day, so a limited sampling of $\\lambda$ is available.\n",
    "    - There is no real prediction step for classes not amongst the two that were picked here.\n",
    "    - 80% of the data will hence invariably be misclassified, meaning the TEST error should be at least 80%\n",
    "    - The training set included only the two classes, and do not have this issue. \n",
    "    \n",
    "Now, we see some expected results. In our training data, our error is relatively low for any value of $\\lambda$, but increases slightly (by the order of $10^{-3}$) as $\\lambda$ grows, meaning our model seeks to overfit to the training set, and suffers from a regularization penalty. By this reasoning alone, $\\lambda=0.01$ would be our optimal term, but this is unlikely.\n",
    "\n",
    "In the test set, however, the error shrinks by a similarly small margin as $\\lambda$ grows, although the error in general appears much higher as previously stated. This suggests that the regularization term is helping the model generalize to test data, and penalizes overfitting. By this reasoning, $\\lambda=1$ is our optimal term. \n",
    "\n",
    "Still, it is concerning that our accuracy score for test data peaks at around 8.3%, when its maximum potential value could be around 20%. Likely, tweaks to the descent algorithm, the objective function, and hyperparameters can improve this result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training on different lambdas \n",
    "lambdas = np.logspace(-2,0,3)\n",
    "train_errs = [] \n",
    "test_preds = []\n",
    "\n",
    "for lmbda in lambdas:\n",
    "    B = fastgradient(B_init, Th_init, lmbda, eta_init)[-1]\n",
    "    err = misclassification(B, X_train, y_train)\n",
    "    train_errs.append(err)\n",
    "    \n",
    "    test_pred = 1/(1+np.exp(-X_test.dot(B))) > 0.5\n",
    "    test_pred = 1*test_pred \n",
    "    test_preds.append(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting test errors and plotting\n",
    "for i in range(len(lambdas)):\n",
    "    s = None \n",
    "    if lambdas[i] == 0.01:\n",
    "        s = '_0_01'\n",
    "    elif lambdas[i] == 0.1:\n",
    "        s = '_0_1'\n",
    "    else:\n",
    "        s = '_1'\n",
    "    \n",
    "    pd.DataFrame(test_preds[i]).to_csv('predictions%s.csv' % s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from Kaggle submissions \n",
    "test_errs = [1-0.07333, 1-0.08000, 1-0.08333]\n",
    "\n",
    "plt.plot(lambdas, test_errs, label='Test Error', marker='o')\n",
    "plt.plot(lambdas, train_errs, label='Train Error', marker='o')\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Regularization Penalty Term')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Best $\\lambda$ by testing data was 1, this code was also run on AWS (the submission for this run can be seen on Kaggle). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
