{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Multi-Class Classification </center>\n",
    "<center> Corinne Jones, TA </center>\n",
    "<center> DATA 558, Spring 2020 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    "As discussed in class, there are a variety of ways you can perform multi-class classification. This lab will show you how you use these methods in Scikit-learn. By the end of the lab, you should:\n",
    "- Be able to describe the differences between one-vs-one, one-vs-rest, and multi-class classification \n",
    "- Understand how to perform one-vs-rest classification yourself\n",
    "- Know various error metrics for multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2 Multi-class methods\n",
    "There are a variety of ways of performing classification when there are are $k>2$ classes. These options include:\n",
    "- Multi-class classification: For some algorithms (like logistic regression and SVM), researchers have developed generalizations to the multi-class case.\n",
    "- Binary one-vs-one classification: In this case, we fit models for all ${k\\choose 2}$ possible pairs of classes. For instance, in one of these models we will label the data in class 0 as +1 and the data in class 1 as -1 and fit a model with the data from these two classes. For prediction, you predict the class of the new point using all ${k\\choose 2}$ classifiers and record which classes it assigns it to. The class it is assigned to most frequently is then our prediction.\n",
    "- Binary one-vs-rest classification: In this case, we fit $k$ models. In each model, data from one of the classes is labeled as +1 and the data from the other classes are labeled -1. For prediction on a new data point, we compute the output of each model on the data point (This output must be continuous). Suppose the model for which the output yields the highest value assigns the label +1 to class $i$. Then we predict that the new point is from class $i$.\n",
    "\n",
    "Let's see how we can use perform multi-class classification in Scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Multinomial Logistic Regression on the Digits dataset\n",
    "In this lab we'll look at how to perform multi-class classification on the Digits dataset. Digits is a dataset consisting of images of handwritten digits. Some of the code below comes from http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of each image: (8, 8)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard scientific Python imports\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "\n",
    "# Import datasets, classifiers and performance metrics\n",
    "import sklearn\n",
    "from sklearn import datasets, linear_model, multiclass, preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# The digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# The data that we are interested in is made of 8x8 images of digits. Let's\n",
    "# have a look at the first 4 images, stored in the `images` attribute of the\n",
    "# dataset.  If we were working from image files, we could load them using\n",
    "# matplotlib.pyplot.imread.  Note that each image must have the same size. For these\n",
    "# images, we know which digit they represent: it is given in the 'target' of\n",
    "# the dataset.\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "for index, (image, label) in enumerate(images_and_labels[:4]):\n",
    "    plt.subplot(1, 4, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('True label: %i' % label)\n",
    "\n",
    "print('Size of each image:', image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in multinomial logistic regression with $k$ classes we have that for classes $j=1,\\dots, k-1$\n",
    "$$ P(y=j|X=x) = \\frac{\\exp(\\beta_{j,0}+\\beta_j^Tx)}{1+\\sum_{m=1}^{k-1}\\exp(\\beta_{m,0} +\\beta_m^Tx)} $$\n",
    "and for class $k$ we have \n",
    "$$ P(y=k|X=x) = \\frac{1}{1+\\sum_{m=1}^{k-1}\\exp(\\beta_{m,0} +\\beta_m^Tx)}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll split the data into training and test sets, since these splits aren't provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(digits.images.reshape(-1, 64), digits.target, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data: (1347, 64)\n",
      "Shape of test data: (450, 64)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of training data:', x_train.shape)\n",
    "print('Shape of test data:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.** Use `LogisticRegressionCV` to fit a multinomial logistic regression model on the training set. Then predict the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9644444444444444"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "mnl = LogisticRegressionCV(multi_class='multinomial', max_iter=500, n_jobs=-1)\n",
    "mnl.fit(x_train, y_train)\n",
    "\n",
    "yhat_test_mnl = mnl.predict(x_test)\n",
    "mnl.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.** Try using the one-vs-rest option instead. How does the accuracy on the test set compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ovr = LogisticRegressionCV(multi_class='ovr', max_iter=500, n_jobs=-1)\n",
    "ovr.fit(x_train, y_train)\n",
    "\n",
    "ovr.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.** Now try one-vs-one using `OneVsOneClassifier` (http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html#sklearn.multiclass.OneVsOneClassifier). How does this accuracy compare to those above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9644444444444444"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrcv = LogisticRegressionCV()\n",
    "ovo = sklearn.multiclass.OneVsOneClassifier(lrcv, n_jobs=-1)\n",
    "\n",
    "ovo.fit(x_train, y_train)\n",
    "ovo.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Do-it-yourself one-vs-rest with binary logistic loss\n",
    "In this section we'll see how to perform one-vs-rest with logistic regression. Since you have already coded $l_2^2$-regularized logistic regression for homework, we'll just use scikit-learn's logistic regression function here. Recall that coding one-vs-rest ourselves requires fitting $k$ classifiers, where $k$ is the number of classes. Also note that when the training set is imbalanced, reweighting the data can sometimes be helpful. You can specify weights with the class_weight option in LogisticRegressionCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.** Fill in the code below to perform one-vs-rest classification using the binary logistic loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.67%\n"
     ]
    }
   ],
   "source": [
    "k = np.max(y_train) + 1  # Number of classes\n",
    "n_test = np.size(x_test, 0)  # Number of examples in the test set\n",
    "est_probs = np.zeros((n_test, k))  # Matrix where we'll store the estimated probabilities\n",
    "lrcv = LogisticRegressionCV(max_iter=500)\n",
    "\n",
    "for i in range(k):\n",
    "    # To do: Convert labels to indicate whether the observation is from class i\n",
    "    y_i = (y_train == i)\n",
    "    # To do: Fit a logistic regression model on class i vs. the other classes\n",
    "    lrcv.fit(x_train, y_i)\n",
    "    # To do: Get and store the estimated probability that each obs is in class i using the predict_proba() method\n",
    "    est_probs[:, i] = lrcv.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# Find the most likely class for each test example\n",
    "predictions = np.argmax(est_probs, axis=1)\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 8, 9, ..., 7, 7, 8])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False, ..., False, False,  True])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_train == 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=10, class_weight=None, cv=None, dual=False,\n",
       "                     fit_intercept=True, intercept_scaling=1.0, l1_ratios=None,\n",
       "                     max_iter=100, multi_class='auto', n_jobs=None,\n",
       "                     penalty='l2', random_state=None, refit=True, scoring=None,\n",
       "                     solver='lbfgs', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_i = (y_train == 0)\n",
    "lrcv.fit(x_train, y_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99999981e-01, 1.85901236e-08],\n",
       "       [1.00000000e+00, 1.32646333e-10],\n",
       "       [1.00000000e+00, 2.63006798e-14],\n",
       "       [9.99999673e-01, 3.27099163e-07],\n",
       "       [9.99815981e-01, 1.84019486e-04],\n",
       "       [9.99999993e-01, 6.56959625e-09],\n",
       "       [1.00000000e+00, 4.46194630e-13],\n",
       "       [9.99971211e-01, 2.87889046e-05],\n",
       "       [9.99999999e-01, 7.18194219e-10],\n",
       "       [9.99975138e-01, 2.48624059e-05],\n",
       "       [1.00000000e+00, 1.05140908e-14],\n",
       "       [9.99989595e-01, 1.04046764e-05],\n",
       "       [9.99996931e-01, 3.06900290e-06],\n",
       "       [9.99998509e-01, 1.49084572e-06],\n",
       "       [9.99999998e-01, 1.65673929e-09],\n",
       "       [9.99999986e-01, 1.36149782e-08],\n",
       "       [1.00000000e+00, 1.34006682e-12],\n",
       "       [6.96181921e-07, 9.99999304e-01],\n",
       "       [1.00000000e+00, 8.26207111e-11],\n",
       "       [9.99999870e-01, 1.29820162e-07],\n",
       "       [9.99998739e-01, 1.26067607e-06],\n",
       "       [9.99925405e-01, 7.45946079e-05],\n",
       "       [9.99999997e-01, 3.30145276e-09],\n",
       "       [9.96596355e-01, 3.40364477e-03],\n",
       "       [1.00000000e+00, 2.12256313e-11],\n",
       "       [9.99999926e-01, 7.36146294e-08],\n",
       "       [9.98948634e-01, 1.05136558e-03],\n",
       "       [9.99999858e-01, 1.42150000e-07],\n",
       "       [1.00000000e+00, 1.33357275e-13],\n",
       "       [9.99995585e-01, 4.41485045e-06],\n",
       "       [9.99880489e-01, 1.19510868e-04],\n",
       "       [9.99999935e-01, 6.49912773e-08],\n",
       "       [9.99990838e-01, 9.16159806e-06],\n",
       "       [9.99990423e-01, 9.57685918e-06],\n",
       "       [9.98643566e-01, 1.35643424e-03],\n",
       "       [1.00000000e+00, 2.90810050e-12],\n",
       "       [1.00000000e+00, 2.50325836e-13],\n",
       "       [9.99999997e-01, 2.64927964e-09],\n",
       "       [1.00028313e-04, 9.99899972e-01],\n",
       "       [1.00000000e+00, 1.07668180e-11],\n",
       "       [9.99999999e-01, 1.27278891e-09],\n",
       "       [9.99923948e-01, 7.60523547e-05],\n",
       "       [1.00000000e+00, 2.81641935e-11],\n",
       "       [9.99999216e-01, 7.84425004e-07],\n",
       "       [9.99999716e-01, 2.84354092e-07],\n",
       "       [6.68916383e-03, 9.93310836e-01],\n",
       "       [9.99999999e-01, 5.97982542e-10],\n",
       "       [9.99991358e-01, 8.64158897e-06],\n",
       "       [9.99999999e-01, 5.24760320e-10],\n",
       "       [1.00000000e+00, 1.17114943e-13],\n",
       "       [9.99967403e-01, 3.25971160e-05],\n",
       "       [9.99999998e-01, 1.58490975e-09],\n",
       "       [9.99999771e-01, 2.28829749e-07],\n",
       "       [9.99974840e-01, 2.51598671e-05],\n",
       "       [1.61021649e-04, 9.99838978e-01],\n",
       "       [2.31228156e-05, 9.99976877e-01],\n",
       "       [9.99999916e-01, 8.35607533e-08],\n",
       "       [1.00000000e+00, 1.89148789e-11],\n",
       "       [9.99974081e-01, 2.59185218e-05],\n",
       "       [1.00000000e+00, 2.47600117e-11],\n",
       "       [2.72677059e-04, 9.99727323e-01],\n",
       "       [1.00000000e+00, 1.18319110e-11],\n",
       "       [9.99999984e-01, 1.61023061e-08],\n",
       "       [9.99976928e-01, 2.30720129e-05],\n",
       "       [1.00000000e+00, 6.25229366e-16],\n",
       "       [9.99998270e-01, 1.72964082e-06],\n",
       "       [9.99999904e-01, 9.58883451e-08],\n",
       "       [9.99997557e-01, 2.44303004e-06],\n",
       "       [9.99850126e-01, 1.49873796e-04],\n",
       "       [1.00000000e+00, 6.34760373e-11],\n",
       "       [9.99999999e-01, 9.92387323e-10],\n",
       "       [1.00000000e+00, 4.60142580e-10],\n",
       "       [9.99999589e-01, 4.11107391e-07],\n",
       "       [1.00000000e+00, 9.86041037e-14],\n",
       "       [1.00000000e+00, 7.08061284e-15],\n",
       "       [9.99999997e-01, 3.23560308e-09],\n",
       "       [9.99999996e-01, 4.14943790e-09],\n",
       "       [9.99999990e-01, 9.59982818e-09],\n",
       "       [9.99963324e-01, 3.66760476e-05],\n",
       "       [9.99999848e-01, 1.51725863e-07],\n",
       "       [1.00000000e+00, 2.35150301e-12],\n",
       "       [9.99999982e-01, 1.84659655e-08],\n",
       "       [9.99999999e-01, 5.64383955e-10],\n",
       "       [1.42914480e-04, 9.99857086e-01],\n",
       "       [9.99957367e-01, 4.26329731e-05],\n",
       "       [9.99995431e-01, 4.56859926e-06],\n",
       "       [1.00000000e+00, 3.64500944e-10],\n",
       "       [1.00000000e+00, 1.57878233e-11],\n",
       "       [1.00000000e+00, 7.91358960e-13],\n",
       "       [1.00000000e+00, 1.09596334e-10],\n",
       "       [9.99976821e-01, 2.31791039e-05],\n",
       "       [9.99999831e-01, 1.69376057e-07],\n",
       "       [1.00000000e+00, 1.82208496e-11],\n",
       "       [1.00000000e+00, 7.45292694e-13],\n",
       "       [1.00000000e+00, 8.77145608e-14],\n",
       "       [1.00000000e+00, 1.34921610e-10],\n",
       "       [9.99999980e-01, 2.00721923e-08],\n",
       "       [9.99999283e-01, 7.17245368e-07],\n",
       "       [1.00000000e+00, 1.99659731e-10],\n",
       "       [9.99846390e-01, 1.53610439e-04],\n",
       "       [9.99999983e-01, 1.69791730e-08],\n",
       "       [9.99990483e-01, 9.51719826e-06],\n",
       "       [1.00000000e+00, 6.29384605e-11],\n",
       "       [9.99999977e-01, 2.30305892e-08],\n",
       "       [9.99974374e-01, 2.56260521e-05],\n",
       "       [5.40896804e-06, 9.99994591e-01],\n",
       "       [9.99249363e-01, 7.50637423e-04],\n",
       "       [1.00000000e+00, 6.80817394e-14],\n",
       "       [9.99985301e-01, 1.46987034e-05],\n",
       "       [9.99999988e-01, 1.20731147e-08],\n",
       "       [9.99999799e-01, 2.01402367e-07],\n",
       "       [9.97802047e-01, 2.19795252e-03],\n",
       "       [9.99999955e-01, 4.48461425e-08],\n",
       "       [9.99997022e-01, 2.97810751e-06],\n",
       "       [1.00000000e+00, 2.11240855e-10],\n",
       "       [9.99999915e-01, 8.51465319e-08],\n",
       "       [9.99999995e-01, 5.10662363e-09],\n",
       "       [2.13632065e-01, 7.86367935e-01],\n",
       "       [1.00000000e+00, 5.39739522e-13],\n",
       "       [9.99999996e-01, 3.58396176e-09],\n",
       "       [9.99999930e-01, 7.01069982e-08],\n",
       "       [9.99999988e-01, 1.24673532e-08],\n",
       "       [9.99997962e-01, 2.03812463e-06],\n",
       "       [9.99907635e-01, 9.23651454e-05],\n",
       "       [1.00000000e+00, 7.48914340e-12],\n",
       "       [9.99992992e-01, 7.00771760e-06],\n",
       "       [1.13223561e-05, 9.99988678e-01],\n",
       "       [9.99998903e-01, 1.09726879e-06],\n",
       "       [1.00000000e+00, 1.88591679e-10],\n",
       "       [9.99999354e-01, 6.45551450e-07],\n",
       "       [1.00000000e+00, 9.86382219e-12],\n",
       "       [9.99970420e-01, 2.95800532e-05],\n",
       "       [9.99986480e-01, 1.35198624e-05],\n",
       "       [9.99999999e-01, 1.10311960e-09],\n",
       "       [9.99998552e-01, 1.44776970e-06],\n",
       "       [9.99748397e-01, 2.51602729e-04],\n",
       "       [1.00000000e+00, 2.51114379e-13],\n",
       "       [1.00000000e+00, 7.37463678e-13],\n",
       "       [9.99999980e-01, 2.02998364e-08],\n",
       "       [9.99919423e-01, 8.05773275e-05],\n",
       "       [1.00000000e+00, 7.01669405e-12],\n",
       "       [9.99999999e-01, 8.08719968e-10],\n",
       "       [1.00000000e+00, 2.75194954e-11],\n",
       "       [9.99999999e-01, 8.72864879e-10],\n",
       "       [9.99987442e-01, 1.25584240e-05],\n",
       "       [1.00000000e+00, 5.82060462e-12],\n",
       "       [1.00000000e+00, 3.89591844e-11],\n",
       "       [1.00000000e+00, 1.60349572e-11],\n",
       "       [1.00000000e+00, 1.80165420e-10],\n",
       "       [9.99999862e-01, 1.37556695e-07],\n",
       "       [9.98972608e-01, 1.02739160e-03],\n",
       "       [9.99999995e-01, 4.61351882e-09],\n",
       "       [9.99988301e-01, 1.16988071e-05],\n",
       "       [1.00000000e+00, 7.41072869e-15],\n",
       "       [9.99999706e-01, 2.94277792e-07],\n",
       "       [1.00000000e+00, 4.44954357e-12],\n",
       "       [9.99797625e-01, 2.02375402e-04],\n",
       "       [1.00000000e+00, 3.39572702e-10],\n",
       "       [9.99996183e-01, 3.81705174e-06],\n",
       "       [1.00000000e+00, 1.15780371e-10],\n",
       "       [9.99999998e-01, 1.53727468e-09],\n",
       "       [1.00000000e+00, 1.13692604e-12],\n",
       "       [9.99985971e-01, 1.40288821e-05],\n",
       "       [9.99999994e-01, 6.30100381e-09],\n",
       "       [9.99999977e-01, 2.32530646e-08],\n",
       "       [2.54218478e-05, 9.99974578e-01],\n",
       "       [1.00000000e+00, 2.12344246e-12],\n",
       "       [9.99999964e-01, 3.62043514e-08],\n",
       "       [9.99932531e-01, 6.74689801e-05],\n",
       "       [9.99999999e-01, 7.83082155e-10],\n",
       "       [1.00000000e+00, 8.75879962e-11],\n",
       "       [1.00000000e+00, 7.20382363e-14],\n",
       "       [2.29386517e-05, 9.99977061e-01],\n",
       "       [9.99997329e-01, 2.67119363e-06],\n",
       "       [1.00000000e+00, 4.23546727e-15],\n",
       "       [9.99999752e-01, 2.48204496e-07],\n",
       "       [1.00000000e+00, 7.27527579e-11],\n",
       "       [1.00000000e+00, 3.02720027e-10],\n",
       "       [9.99980330e-01, 1.96695469e-05],\n",
       "       [1.00000000e+00, 1.69680100e-10],\n",
       "       [9.99324358e-01, 6.75641776e-04],\n",
       "       [9.99999385e-01, 6.15033828e-07],\n",
       "       [9.99999207e-01, 7.93080869e-07],\n",
       "       [9.99999776e-01, 2.23871306e-07],\n",
       "       [2.97698360e-01, 7.02301640e-01],\n",
       "       [9.99999991e-01, 9.32676158e-09],\n",
       "       [9.99999535e-01, 4.64785538e-07],\n",
       "       [9.99999999e-01, 5.47455998e-10],\n",
       "       [4.20186661e-07, 9.99999580e-01],\n",
       "       [9.99999999e-01, 5.88049962e-10],\n",
       "       [9.99999963e-01, 3.71970972e-08],\n",
       "       [9.99998823e-01, 1.17680603e-06],\n",
       "       [9.99999999e-01, 8.75595410e-10],\n",
       "       [9.99999951e-01, 4.88571052e-08],\n",
       "       [9.53362234e-07, 9.99999047e-01],\n",
       "       [1.00000000e+00, 1.36931602e-13],\n",
       "       [1.00000000e+00, 9.79279352e-11],\n",
       "       [9.99969671e-01, 3.03285914e-05],\n",
       "       [9.99975202e-01, 2.47979405e-05],\n",
       "       [9.99999982e-01, 1.79326517e-08],\n",
       "       [9.99999117e-01, 8.82561998e-07],\n",
       "       [6.05681332e-05, 9.99939432e-01],\n",
       "       [1.00000000e+00, 1.68992494e-11],\n",
       "       [9.99999700e-01, 3.00015436e-07],\n",
       "       [9.99999993e-01, 6.77290133e-09],\n",
       "       [1.00000000e+00, 8.80470313e-12],\n",
       "       [1.00000000e+00, 4.27873577e-13],\n",
       "       [1.00000000e+00, 4.35943881e-10],\n",
       "       [9.12290921e-05, 9.99908771e-01],\n",
       "       [9.93035301e-01, 6.96469905e-03],\n",
       "       [9.99999993e-01, 7.08434630e-09],\n",
       "       [9.99999980e-01, 2.03195797e-08],\n",
       "       [9.99798133e-01, 2.01867217e-04],\n",
       "       [9.99943317e-01, 5.66832087e-05],\n",
       "       [9.99999997e-01, 3.41809082e-09],\n",
       "       [9.99999991e-01, 8.69260040e-09],\n",
       "       [1.00000000e+00, 7.06743305e-13],\n",
       "       [9.99999995e-01, 5.06823217e-09],\n",
       "       [9.99999900e-01, 9.99425530e-08],\n",
       "       [9.99986218e-01, 1.37816959e-05],\n",
       "       [9.99996497e-01, 3.50328449e-06],\n",
       "       [9.99999996e-01, 3.60564872e-09],\n",
       "       [9.99661112e-01, 3.38887646e-04],\n",
       "       [9.99999996e-01, 4.16139040e-09],\n",
       "       [1.00000000e+00, 1.67770355e-10],\n",
       "       [9.99994436e-01, 5.56371512e-06],\n",
       "       [1.00000000e+00, 7.54392908e-11],\n",
       "       [9.99999987e-01, 1.29469742e-08],\n",
       "       [1.00000000e+00, 2.48201606e-10],\n",
       "       [9.99999964e-01, 3.64489377e-08],\n",
       "       [9.99999408e-01, 5.92149865e-07],\n",
       "       [9.99994687e-01, 5.31332992e-06],\n",
       "       [9.99999921e-01, 7.90992544e-08],\n",
       "       [9.99997163e-01, 2.83742982e-06],\n",
       "       [9.99998995e-01, 1.00453534e-06],\n",
       "       [5.51357703e-01, 4.48642297e-01],\n",
       "       [9.99999954e-01, 4.59566474e-08],\n",
       "       [1.47598432e-05, 9.99985240e-01],\n",
       "       [9.99999999e-01, 5.20259362e-10],\n",
       "       [9.99999382e-01, 6.18337796e-07],\n",
       "       [1.00000000e+00, 7.86564140e-14],\n",
       "       [1.00000000e+00, 5.02423095e-13],\n",
       "       [9.97923148e-01, 2.07685249e-03],\n",
       "       [1.00000000e+00, 1.61946785e-12],\n",
       "       [9.99983530e-01, 1.64695508e-05],\n",
       "       [1.00000000e+00, 7.36365616e-12],\n",
       "       [9.99999993e-01, 6.62989845e-09],\n",
       "       [9.99999548e-01, 4.51967197e-07],\n",
       "       [9.99999958e-01, 4.21623389e-08],\n",
       "       [1.00000000e+00, 3.76549992e-10],\n",
       "       [9.99999795e-01, 2.05271258e-07],\n",
       "       [9.99999992e-01, 7.76438400e-09],\n",
       "       [9.99999997e-01, 3.04699545e-09],\n",
       "       [1.00000000e+00, 1.27923511e-11],\n",
       "       [1.00000000e+00, 7.12599357e-12],\n",
       "       [9.99999982e-01, 1.77008256e-08],\n",
       "       [9.99999991e-01, 8.75260605e-09],\n",
       "       [1.00000000e+00, 1.63962621e-10],\n",
       "       [9.99999821e-01, 1.78881777e-07],\n",
       "       [1.16809275e-03, 9.98831907e-01],\n",
       "       [9.99513576e-01, 4.86423584e-04],\n",
       "       [1.00000000e+00, 8.15160085e-14],\n",
       "       [9.99958585e-01, 4.14153358e-05],\n",
       "       [9.99999985e-01, 1.47995337e-08],\n",
       "       [9.99365144e-01, 6.34855820e-04],\n",
       "       [1.00000000e+00, 5.99493416e-12],\n",
       "       [9.99999996e-01, 3.73362638e-09],\n",
       "       [9.99999998e-01, 1.62526348e-09],\n",
       "       [1.00000000e+00, 6.41084823e-12],\n",
       "       [2.95329710e-01, 7.04670290e-01],\n",
       "       [1.00000000e+00, 1.05812242e-10],\n",
       "       [9.99999518e-01, 4.82173480e-07],\n",
       "       [9.99995866e-01, 4.13366773e-06],\n",
       "       [9.99967865e-01, 3.21350106e-05],\n",
       "       [9.99999999e-01, 7.77254034e-10],\n",
       "       [9.99999995e-01, 5.38513960e-09],\n",
       "       [9.99999919e-01, 8.06085996e-08],\n",
       "       [9.99999999e-01, 1.47836867e-09],\n",
       "       [1.00000000e+00, 1.05911156e-10],\n",
       "       [9.99999998e-01, 2.29242617e-09],\n",
       "       [9.99999218e-01, 7.81674884e-07],\n",
       "       [9.99998908e-01, 1.09229169e-06],\n",
       "       [1.00000000e+00, 2.78072032e-10],\n",
       "       [1.00000000e+00, 1.53369499e-14],\n",
       "       [1.00000000e+00, 9.81963341e-13],\n",
       "       [1.00000000e+00, 7.19771679e-11],\n",
       "       [1.00000000e+00, 4.20797580e-10],\n",
       "       [9.99999999e-01, 1.08381476e-09],\n",
       "       [1.00000000e+00, 2.39158150e-11],\n",
       "       [9.99992989e-01, 7.01121313e-06],\n",
       "       [9.99990772e-01, 9.22826159e-06],\n",
       "       [9.99998905e-01, 1.09471692e-06],\n",
       "       [9.99999183e-01, 8.17138378e-07],\n",
       "       [9.99973145e-01, 2.68545320e-05],\n",
       "       [9.99237180e-01, 7.62820248e-04],\n",
       "       [1.00000000e+00, 2.72140291e-10],\n",
       "       [9.94417984e-01, 5.58201642e-03],\n",
       "       [9.99999924e-01, 7.58472333e-08],\n",
       "       [1.00000000e+00, 1.15637114e-13],\n",
       "       [9.99999981e-01, 1.91600023e-08],\n",
       "       [1.95592766e-05, 9.99980441e-01],\n",
       "       [9.99998540e-01, 1.45991889e-06],\n",
       "       [9.93106498e-01, 6.89350190e-03],\n",
       "       [1.00000000e+00, 2.98924077e-11],\n",
       "       [1.14488749e-06, 9.99998855e-01],\n",
       "       [9.99994018e-01, 5.98220148e-06],\n",
       "       [9.99962812e-01, 3.71883032e-05],\n",
       "       [1.12839874e-02, 9.88716013e-01],\n",
       "       [9.99999886e-01, 1.13700519e-07],\n",
       "       [9.99999978e-01, 2.23352889e-08],\n",
       "       [8.50887154e-06, 9.99991491e-01],\n",
       "       [9.99999993e-01, 7.21981325e-09],\n",
       "       [9.99999999e-01, 1.03466921e-09],\n",
       "       [9.99999998e-01, 1.99084343e-09],\n",
       "       [9.99995743e-01, 4.25747851e-06],\n",
       "       [9.99997837e-01, 2.16304224e-06],\n",
       "       [9.99999992e-01, 7.89237736e-09],\n",
       "       [9.99999999e-01, 8.76485283e-10],\n",
       "       [1.00000000e+00, 1.76266875e-10],\n",
       "       [9.99963560e-01, 3.64398340e-05],\n",
       "       [9.99999991e-01, 8.58476872e-09],\n",
       "       [9.99999814e-01, 1.85807278e-07],\n",
       "       [1.00000000e+00, 6.81654563e-11],\n",
       "       [9.99989448e-01, 1.05524415e-05],\n",
       "       [1.00000000e+00, 1.80390510e-10],\n",
       "       [9.99999997e-01, 2.72517946e-09],\n",
       "       [9.99999869e-01, 1.30688697e-07],\n",
       "       [1.00000000e+00, 8.43289895e-14],\n",
       "       [5.53723818e-06, 9.99994463e-01],\n",
       "       [1.00000000e+00, 2.58799955e-11],\n",
       "       [9.99999999e-01, 5.81600063e-10],\n",
       "       [9.99322178e-01, 6.77822102e-04],\n",
       "       [9.99999590e-01, 4.10056605e-07],\n",
       "       [9.99996338e-01, 3.66189319e-06],\n",
       "       [9.99887409e-01, 1.12590519e-04],\n",
       "       [9.99437367e-01, 5.62633020e-04],\n",
       "       [9.99999926e-01, 7.41620429e-08],\n",
       "       [9.99999802e-01, 1.97926394e-07],\n",
       "       [9.99999987e-01, 1.31978942e-08],\n",
       "       [1.00000000e+00, 2.23409970e-13],\n",
       "       [1.00000000e+00, 1.58508139e-10],\n",
       "       [1.00000000e+00, 4.25955672e-16],\n",
       "       [9.99986764e-01, 1.32364720e-05],\n",
       "       [1.00000000e+00, 2.16289061e-12],\n",
       "       [9.99999995e-01, 5.01570128e-09],\n",
       "       [1.28515353e-04, 9.99871485e-01],\n",
       "       [9.99998515e-01, 1.48543083e-06],\n",
       "       [9.99999878e-01, 1.21610532e-07],\n",
       "       [1.00000000e+00, 8.96899438e-13],\n",
       "       [1.00000000e+00, 2.66717055e-12],\n",
       "       [9.99999897e-01, 1.02909108e-07],\n",
       "       [5.43801676e-04, 9.99456198e-01],\n",
       "       [9.99999995e-01, 4.86726270e-09],\n",
       "       [9.93325873e-01, 6.67412707e-03],\n",
       "       [1.00383866e-02, 9.89961613e-01],\n",
       "       [9.99999974e-01, 2.55865253e-08],\n",
       "       [9.99994700e-01, 5.30014178e-06],\n",
       "       [9.99999999e-01, 9.34974202e-10],\n",
       "       [1.00000000e+00, 5.35158619e-11],\n",
       "       [9.99999618e-01, 3.81765094e-07],\n",
       "       [9.99999855e-01, 1.44721138e-07],\n",
       "       [9.99999999e-01, 1.45039544e-09],\n",
       "       [9.99991361e-01, 8.63892557e-06],\n",
       "       [9.99991166e-01, 8.83410362e-06],\n",
       "       [9.99881760e-01, 1.18240178e-04],\n",
       "       [9.99999999e-01, 5.76514843e-10],\n",
       "       [1.00000000e+00, 7.84252026e-11],\n",
       "       [9.99999994e-01, 6.09144213e-09],\n",
       "       [9.99999993e-01, 7.20432931e-09],\n",
       "       [2.40481097e-05, 9.99975952e-01],\n",
       "       [1.00000000e+00, 4.71206654e-10],\n",
       "       [9.99932479e-01, 6.75208049e-05],\n",
       "       [1.00000000e+00, 3.38900885e-10],\n",
       "       [1.00000000e+00, 5.77669624e-11],\n",
       "       [1.00000000e+00, 6.80227482e-11],\n",
       "       [9.99999999e-01, 8.02500731e-10],\n",
       "       [5.29337449e-05, 9.99947066e-01],\n",
       "       [9.99999997e-01, 2.72543047e-09],\n",
       "       [9.99999825e-01, 1.74514115e-07],\n",
       "       [9.99999990e-01, 1.02732313e-08],\n",
       "       [1.00000000e+00, 1.15977519e-14],\n",
       "       [9.99999992e-01, 7.68471642e-09],\n",
       "       [1.00000000e+00, 5.06426704e-11],\n",
       "       [7.66735246e-07, 9.99999233e-01],\n",
       "       [6.17477518e-05, 9.99938252e-01],\n",
       "       [1.00000000e+00, 3.53777293e-14],\n",
       "       [9.99999080e-01, 9.20129642e-07],\n",
       "       [2.56708906e-05, 9.99974329e-01],\n",
       "       [9.99999998e-01, 2.41110948e-09],\n",
       "       [1.00000000e+00, 2.03617758e-11],\n",
       "       [9.99955868e-01, 4.41319109e-05],\n",
       "       [1.00000000e+00, 2.51479604e-10],\n",
       "       [1.96883975e-01, 8.03116025e-01],\n",
       "       [9.98802537e-01, 1.19746261e-03],\n",
       "       [9.99758576e-01, 2.41423918e-04],\n",
       "       [1.00000000e+00, 2.42013127e-12],\n",
       "       [1.00000000e+00, 1.41129983e-13],\n",
       "       [9.99999590e-01, 4.10289855e-07],\n",
       "       [1.00000000e+00, 1.89872001e-11],\n",
       "       [9.99992604e-01, 7.39573160e-06],\n",
       "       [9.99999999e-01, 6.79658092e-10],\n",
       "       [9.99973841e-01, 2.61591376e-05],\n",
       "       [9.99999985e-01, 1.46251982e-08],\n",
       "       [9.99999977e-01, 2.27226462e-08],\n",
       "       [9.99886637e-01, 1.13363421e-04],\n",
       "       [1.00000000e+00, 1.11562332e-11],\n",
       "       [1.00000000e+00, 1.89421734e-11],\n",
       "       [9.99999910e-01, 9.04703322e-08],\n",
       "       [1.00000000e+00, 6.98658849e-11],\n",
       "       [1.00000000e+00, 8.77162388e-12],\n",
       "       [9.99953968e-01, 4.60320954e-05],\n",
       "       [1.00000000e+00, 2.84585423e-11],\n",
       "       [5.74205771e-06, 9.99994258e-01],\n",
       "       [9.99999997e-01, 3.46249877e-09],\n",
       "       [1.00000000e+00, 7.02519861e-11],\n",
       "       [9.99999997e-01, 3.11990828e-09],\n",
       "       [1.00000000e+00, 3.36708889e-12],\n",
       "       [9.99999996e-01, 4.11211090e-09],\n",
       "       [9.99999966e-01, 3.35731544e-08],\n",
       "       [9.99982882e-01, 1.71176267e-05],\n",
       "       [1.00000000e+00, 4.50305020e-13],\n",
       "       [9.99999975e-01, 2.52545886e-08],\n",
       "       [9.99999985e-01, 1.51676972e-08],\n",
       "       [1.25540847e-03, 9.98744592e-01],\n",
       "       [9.46651804e-05, 9.99905335e-01],\n",
       "       [9.99999647e-01, 3.52960694e-07],\n",
       "       [1.18705310e-05, 9.99988129e-01],\n",
       "       [9.99999957e-01, 4.28996295e-08],\n",
       "       [9.99999290e-01, 7.09613670e-07],\n",
       "       [9.99897618e-01, 1.02381943e-04],\n",
       "       [9.99995146e-01, 4.85380082e-06],\n",
       "       [1.00000000e+00, 1.81003098e-11],\n",
       "       [9.99953986e-01, 4.60136633e-05],\n",
       "       [1.00000000e+00, 6.03936937e-13],\n",
       "       [1.00000000e+00, 7.78351123e-13],\n",
       "       [9.99998727e-01, 1.27262840e-06],\n",
       "       [9.99750294e-01, 2.49706069e-04],\n",
       "       [9.99999991e-01, 8.70691529e-09],\n",
       "       [1.00000000e+00, 2.26784612e-12],\n",
       "       [9.99996265e-01, 3.73454108e-06],\n",
       "       [9.99999954e-01, 4.60748315e-08],\n",
       "       [9.99999992e-01, 7.90592094e-09],\n",
       "       [9.99999259e-01, 7.40839063e-07],\n",
       "       [9.99999997e-01, 3.06433866e-09],\n",
       "       [1.00000000e+00, 4.83353027e-12],\n",
       "       [9.99999642e-01, 3.58073192e-07],\n",
       "       [1.00000000e+00, 3.46222907e-14],\n",
       "       [9.99999524e-01, 4.75926644e-07],\n",
       "       [1.00000000e+00, 4.26242713e-15],\n",
       "       [9.99999178e-01, 8.21613298e-07]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrcv.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Error metrics\n",
    "There are various ways to assess the quality of predictions from multi-class classification. These include:\n",
    "- Confusion matrix: A matrix whose $(i,j)$th entry represents the number of observations that are known to be in class i but were assigned to class j\n",
    "- Misclassification error: The fraction of observations that are misclassified\n",
    "- Top-1 accuracy: The fraction of observations for which the top prediction for that observation is correct\n",
    "- Top-k accuracy: The fraction of observations for which the true label is in the top k predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.** Suppose you have 5 observations $x_1, x_2, x_3, x_4, x_5$ total across three classes, A, B, and C. Your classifier told you the following:\n",
    "- $P(A|x_1) > P(C|x_1) > P(B|x_1)$\n",
    "- $P(C|x_2) > P(A|x_2) > P(B|x_2)$\n",
    "- $P(A|x_3) > P(C|x_3) > P(B|x_3)$\n",
    "- $P(B|x_4) > P(C|x_4) > P(A|x_4)$\n",
    "- $P(A|x_5) > P(B|x_5) > P(C|x_5)$ \n",
    "\n",
    "In reality, $x_1$ and $x_2$ were from class A, $x_3$ and $x_4$ were from class B, and $x_5$ was from class C. Compute the following:\n",
    "1. Confusion matrix\n",
    "2. Misclassification error\n",
    "3. Top-1 accuracy\n",
    "4. Top-2 accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at these on the results from the multinomial logistic regression example above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6.** Compute the confusion matrix for the multinomial logistic example in Section 2.1. Reference: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[37,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0, 40,  0,  0,  0,  0,  1,  0,  1,  1],\n",
       "       [ 0,  0, 44,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0, 44,  0,  0,  0,  0,  1,  0],\n",
       "       [ 0,  0,  0,  0, 36,  0,  0,  2,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0, 46,  0,  0,  0,  2],\n",
       "       [ 0,  1,  0,  0,  0,  0, 51,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  1,  0,  0, 47,  0,  0],\n",
       "       [ 0,  3,  1,  0,  0,  0,  0,  0, 44,  0],\n",
       "       [ 0,  0,  0,  0,  0,  1,  0,  1,  0, 45]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, yhat_test_mnl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above confusion matrix, the rows represent the truth, while the columns represent the predictions. We can see that most of the time the classifier is correct. \n",
    "\n",
    "Now let's look at the other performance measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7.** Compute the top-1 accuracy and misclassification error for the multinomial logistic example in Section 2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9644444444444444"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = np.mean(y_test == yhat_test_mnl)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03555555555555556"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassification_error = 1-accuracy\n",
    "misclassification_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8.** Compute top-2 accuracy for the misclassification error for the multinomial logistic example in Section 2.1. Scikit-learn doesn't appear to have a built-in function to compute it, so compute it \"by hand\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9888888888888889\n"
     ]
    }
   ],
   "source": [
    "probs = mnl.predict_proba(x_test)\n",
    "n = len(x_test)\n",
    "top2_accuracy = 0\n",
    "for i in range(n):\n",
    "    top2 = np.argsort(probs[i, :])[-2:]\n",
    "    if y_test[i] in top2:\n",
    "        top2_accuracy += 1\n",
    "\n",
    "top2_accuracy /= n\n",
    "print(top2_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.4 Imbalanced classes\n",
    "When the number of observations in one class is much different than the number of observations in the other class(es), it is often beneficial to reweight the data. Suppose the labels $y_i$ are $\\pm 1$. Then we can write the usual objective function as \n",
    "\n",
    "$$ \\frac{1}{n} \\sum_{i=1}^n L(x_i, y_i; \\beta) = \\frac{1}{n} \\sum_{i:y_i=+1} L(x_i, y_i; \\beta) + \\frac{1}{n} \\sum_{i:y_i=-1} L(x_i, y_i; \\beta). $$\n",
    "\n",
    "From this, we can see that if, e.g., there are many more observations with $y_i=-1$ than with $y_i=+1$, then the negative observations are weighted more heavily than the positive observations. If we want to weight the positive and negative classes equally, we can take as our objective function\n",
    "\n",
    "$$ \\frac{1/2}{n_+} \\sum_{i:y_i=+1} L(x_i, y_i; \\beta) + \\frac{1/2}{n_-} \\sum_{i:y_i=-1} L(x_i, y_i; \\beta), $$ \n",
    "where $n_+=\\left\\vert\\{ i:y_i=+1\\}\\right\\vert$ (the number of observations with $y_i=+1$) and $n_-=\\left\\vert\\{ i:y_i=-1\\}\\right\\vert$ (the number of observations with $y_i=-1$). Even better, we can optimize over a class imbalance parameter $\\rho$:\n",
    "\n",
    "$$ \\frac{\\rho}{n_+} \\sum_{i:y_i=+1} L(x_i, y_i; \\beta) + \\frac{1-\\rho}{n_-} \\sum_{i:y_i=-1} L(x_i, y_i; \\beta). $$\n",
    "\n",
    "When performing classification with $k$ classes, $j=1,\\dots, k$, optimizing one imbalance parameter per class is too computationally demanding, so we typically just weight each class equally:\n",
    "\n",
    "$$ \\frac{1}{k}\\sum_{j=1}^k \\frac{1}{n_j} \\sum_{i:y_i=j} L(x_i, y_i; \\beta), $$\n",
    "where $n_j=\\left\\vert\\{ i:y_i=j\\}\\right\\vert$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 9.** Redo exercise 2 using the `class_weight=balanced` option. Does the performance of the classifier improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9622222222222222"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ovr = LogisticRegressionCV(multi_class='ovr', max_iter=500, n_jobs=-1, class_weight='balanced')\n",
    "ovr.fit(x_train, y_train)\n",
    "\n",
    "ovr.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
